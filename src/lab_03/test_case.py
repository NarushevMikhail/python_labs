from lab_03.text import normalize, tokenize, count_freq, top_n


print("normalize:")
print(normalize("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t"))
print(normalize("—ë–∂–∏–∫, –Å–ª–∫–∞"))
print(normalize("Hello\r\nWorld"))
print(normalize("  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  "))

print("tokenize:")
print(tokenize("–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"))
print(tokenize("hello,world!!!"))
print(tokenize("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –Ω–µ –∫—Ä—É—Ç–æ"))
print(tokenize("2125 –≥–æ–¥"))
print(tokenize("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ"))

print("count_freq:")
print(count_freq(["a","b","a","c","b","a"]))
print(count_freq(["bb","aa","bb","aa","cc"]))
freq = count_freq(["a","b","a","c","b","a"])
print(top_n(freq, 2))
freq_1 = count_freq(["bb","aa","bb","aa","cc"])
print(top_n(freq_1, 2))